{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "S_XUFOaMhF1e",
        "GHzD8as6hMq8",
        "TdA_5dsNhor1",
        "cYqpNYal8SlF",
        "7817eAvj8Jqr",
        "5C3F7uSu8YHs",
        "z_VadeKoJsq2",
        "_NpgKzWqKKP4",
        "2E1I8ULdKXsd",
        "QhPndE6i8cv1",
        "yKqVUYHG08_r",
        "BnUX2XgYMe7E",
        "jx13jwtEMBgq"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPbT6Nqbv8U7YPCgJoKSIEi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pauloprsdesouza/recsys-twitter-social-capital/blob/dev-colab/RecSysTwitterSocialCapital.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize Dependencies"
      ],
      "metadata": {
        "id": "S_XUFOaMhF1e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJz0N5vzg2u2"
      },
      "outputs": [],
      "source": [
        "pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-twitter-v2"
      ],
      "metadata": {
        "id": "xrCL3dDCgdj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "Qjn4aNe0gfSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspellchecker"
      ],
      "metadata": {
        "id": "PONyVjoxo-oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import necessary libraries"
      ],
      "metadata": {
        "id": "GHzD8as6hMq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import floresta\n",
        "import datetime\n",
        "import emoji\n",
        "import math\n",
        "from pytwitter.models import User, Tweet, TweetEntities, TweetEntitiesUrl, TweetPublicMetrics, TweetEntitiesMention, TweetEntitiesHashtag, TweetEntitiesAnnotation\n",
        "from pytwitter import Api\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from spellchecker import SpellChecker\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## Nltk Dependencies\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('floresta')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "stemmer = SnowballStemmer('portuguese')\n",
        "stop_words = set(stopwords.words('portuguese'))\n",
        "\n",
        "## Bert Configuration\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "iWL3PuCYhRl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Api Bearer Definition"
      ],
      "metadata": {
        "id": "TdA_5dsNhor1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api = Api(bearer_token=\"AAAAAAAAAAAAAAAAAAAAAMhqlAEAAAAA4Pqzn354Z5nlkP5lKaW98vzlVlA%3D7GIA03xacVKdFYTFg7qmgvWTZThpa2FFd4SNPUqP7uPK7Xjue5\")"
      ],
      "metadata": {
        "id": "5478U2QbhsAx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Collect Data"
      ],
      "metadata": {
        "id": "r_SIieVBhzVW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial parameters"
      ],
      "metadata": {
        "id": "bkyFkX-Mn7qN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "usersById = {}\n",
        "usersByUserName = {}\n",
        "usersEngagementData = {}\n",
        "number_results = 100"
      ],
      "metadata": {
        "id": "l5bGruFYn6R9"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get user from Twitter"
      ],
      "metadata": {
        "id": "_laJCUENjbkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_mentioned(username):\n",
        "  try:\n",
        "    user = api.get_user(username=username) \n",
        "  except:\n",
        "    usersByUserName[username] = 0 \n",
        "  else:\n",
        "    usersEngagementData[user.id] = {'tweets':[], 'mentions':[], 'replies':[]}\n",
        "    user_strength = user_engagement_strength_score(user, usersEngagementData, number_results)\n",
        "    usersById[user.id] = user_strength\n",
        "    usersByUserName[user.username] = user_strength"
      ],
      "metadata": {
        "id": "TD_2lHK8jjv1"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_data(subject):\n",
        "    public_tweets = api.search_tweets(query=f\"{subject} lang:pt has:hashtags -is:retweet has:media\", \n",
        "                                      expansions=[\"referenced_tweets.id.author_id\",\"in_reply_to_user_id\",\"attachments.media_keys\",\"author_id\",\"entities.mentions.username\"], \n",
        "                                      user_fields=[\"created_at\",\"entities\",\"id\",\"location\",\"name\",\"pinned_tweet_id\",\"profile_image_url\",\"protected\",\"public_metrics\",\"url\",\"username\",\"verified\"],\n",
        "                                      tweet_fields=[\"attachments\",\"author_id\",\"context_annotations\",\"created_at\",\"entities\",\"geo\",\"in_reply_to_user_id\",\"lang\",\"public_metrics\",\"reply_settings\",\"source\"], \n",
        "                                      max_results=100)\n",
        "    \n",
        "    \n",
        "    \n",
        "    for user in public_tweets.includes.users:\n",
        "       usersEngagementData[user.id] = {'tweets':[], 'mentions':[], 'replies':[]}\n",
        "       user_strength = user_engagement_strength_score(user, usersEngagementData, number_results)\n",
        "       usersById[user.id] = user_strength\n",
        "       usersByUserName[user.username] = user_strength\n",
        "\n",
        "    for tweet in public_tweets.data:\n",
        "        user_id = tweet.author_id\n",
        "        if user_id not in usersById:\n",
        "            usersEngagementData[user.id] = {'tweets':[], 'mentions':[], 'replies':[]}\n",
        "            user = api.get_user(user_id)\n",
        "            user_strength = user_engagement_strength_score(user, usersEngagementData, number_results)\n",
        "            usersById[user_id] = user_strength\n",
        "            usersByUserName[user.username] = user_strength \n",
        "\n",
        "    return public_tweets, usersById, usersByUserName"
      ],
      "metadata": {
        "id": "-pbQ5gKqh9DX"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Text Analysis"
      ],
      "metadata": {
        "id": "Q4aTV9stiGQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Pos from Wordnet"
      ],
      "metadata": {
        "id": "cYqpNYal8SlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to get the WordNet POS for a given token\n",
        "def get_wordnet_pos(token):\n",
        "    tag = nltk.pos_tag([token])[0][1][0].upper()\n",
        "    if tag == 'N':\n",
        "        return wordnet.NOUN\n",
        "    elif tag == 'V':\n",
        "        return wordnet.VERB\n",
        "    elif tag == 'R':\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN"
      ],
      "metadata": {
        "id": "YQPgX_WIvSyN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Desanbiguation Process"
      ],
      "metadata": {
        "id": "7817eAvj8Jqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def disambiguate_word(word, context):\n",
        "    synset = nltk.wsd.lesk(context, word)\n",
        "    if synset is not None:\n",
        "        return synset.name().split('.')[0]\n",
        "    else:\n",
        "        return word"
      ],
      "metadata": {
        "id": "7M3yRJ0BwqtG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Synonimize Process"
      ],
      "metadata": {
        "id": "5C3F7uSu8YHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_synonyms(word):\n",
        "    synonyms = set()\n",
        "    for synset in wordnet.synsets(word):\n",
        "        for lemma in synset.lemmas():\n",
        "            synonyms.add(lemma.name())\n",
        "    return list(synonyms)"
      ],
      "metadata": {
        "id": "zJjg54MWx4Yc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis"
      ],
      "metadata": {
        "id": "z_VadeKoJsq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentiment_score(text):\n",
        "    # Use BERT to calculate the sentiment score of the tweet text\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
        "    inputs.to(device)\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    sentiment_score = probs[0][1].item() - probs[0][0].item()\n",
        "\n",
        "    return sentiment_score"
      ],
      "metadata": {
        "id": "PyvAvo8fJu3A"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diversity Score (TTR)"
      ],
      "metadata": {
        "id": "_NpgKzWqKKP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_diversity_score(words):\n",
        "    # Use a measure of lexical diversity to calculate the diversity score of the tweet text\n",
        "    # For example, using the type-token ratio (TTR) metric\n",
        "    num_words = len(words)\n",
        "    num_unique_words = len(set(words))\n",
        "    diversity_score = num_unique_words / num_words if num_words > 0 else 0\n",
        "\n",
        "    return diversity_score"
      ],
      "metadata": {
        "id": "SbnIyjz-KNXt"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Context Analysis"
      ],
      "metadata": {
        "id": "2E1I8ULdKXsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_context_score(text, topic_keywords):\n",
        "    # Calculate the context score of the tweet based on its relevance to the given topic keywords\n",
        "    # For example, using the cosine similarity between the tweet and the topic keywords\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    corpus = [text] + topic_keywords\n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "    cosine_similarities = np.dot(X[0], X[1:].T).toarray()[0]\n",
        "    context_score = max(cosine_similarities) if len(cosine_similarities) > 0 else 0\n",
        "\n",
        "    return context_score"
      ],
      "metadata": {
        "id": "Vl1FxwfuKauX"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing a text"
      ],
      "metadata": {
        "id": "QhPndE6i8cv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_processing_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Remove Emojis\n",
        "    text = emoji.replace_emoji(text, '')\n",
        "    \n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "    # Remove mentions\n",
        "    text = re.sub(r'@\\S+', '', text)\n",
        "\n",
        "    # Remove hashtags\n",
        "    text = re.sub(r'#\\S+', '', text)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text, language='portuguese')\n",
        "\n",
        "    # Lemmatize the tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Lemmatize the tokens\n",
        "    lemmatized_tokens = []\n",
        "    for token in tokens:\n",
        "        pos = get_wordnet_pos(token)\n",
        "        lemma = lemmatizer.lemmatize(token, pos=pos)\n",
        "        lemmatized_tokens.append(lemma)\n",
        "\n",
        "    # Join the lemmatized tokens to form the tweet text again\n",
        "    lemmatized_tweet = ' '.join(lemmatized_tokens)\n",
        "\n",
        "    # Desanbiguation Process\n",
        "    disambiguated_tokens = [disambiguate_word(token, text) for token in lemmatized_tokens]\n",
        "\n",
        "    # Synonimization Process\n",
        "    synonymized_tokens = []\n",
        "    for token in disambiguated_tokens:\n",
        "        synonyms = get_synonyms(token)\n",
        "        if synonyms:\n",
        "            synonymized_tokens.append(max(synonyms, key=len))\n",
        "        else:\n",
        "            synonymized_tokens.append(token)\n",
        "\n",
        "    # Remove stop words and punctuation\n",
        "    cleaned_tokens = [token for token in synonymized_tokens if token not in stop_words and token.isalnum()]\n",
        "\n",
        "    # Re-tokenize the cleaned tokens\n",
        "    cleaned_tweet = ' '.join(cleaned_tokens)\n",
        "\n",
        "    return cleaned_tokens"
      ],
      "metadata": {
        "id": "eeOQuAF_iKhg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyze Data"
      ],
      "metadata": {
        "id": "yKqVUYHG08_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_data(data):\n",
        "  num_tweets = len(data)\n",
        "  num_followers = data['followers'].sum()\n",
        "  num_likes = data['likes'].sum()\n",
        "  num_retweets = data['retweets'].sum()\n",
        "\n",
        "  # Create visualizations\n",
        "  plt.hist(data['likes'])\n",
        "  plt.title('Distribution of Likes')\n",
        "  plt.xlabel('Number of Likes')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.show()\n",
        "\n",
        "  plt.boxplot(data['followers'])\n",
        "  plt.title('Distribution of Followers')\n",
        "  plt.ylabel('Number of Followers')\n",
        "  plt.show()\n",
        "\n",
        "  # Identify common words and hashtags\n",
        "  words = {}\n",
        "  for tweet in data['text']:\n",
        "      for word in tweet.split():\n",
        "          if word.startswith('#'):\n",
        "              words[word] = words.get(word, 0) + 1\n",
        "          else:\n",
        "              words[word.lower()] = words.get(word.lower(), 0) + 1\n",
        "  word_freq = pd.DataFrame.from_dict(words, orient='index', columns=['frequency'])\n",
        "  word_freq.sort_values(by='frequency', ascending=False, inplace=True)\n",
        "  word_freq.head()\n",
        "\n",
        "  # Analyze temporal patterns\n",
        "  data['created_at'] = pd.to_datetime(data['created_at'])\n",
        "  tweets_per_day = data.groupby(pd.Grouper(key='created_at', freq='D')).size()\n",
        "  plt.plot(tweets_per_day)\n",
        "  plt.title('Number of Tweets Posted per Day')\n",
        "  plt.xlabel('Date')\n",
        "  plt.ylabel('Number of Tweets')\n",
        "  plt.show()\n",
        "\n",
        "  # Explore relationships between variables\n",
        "  plt.scatter(data['followers'], data['likes'])\n",
        "  plt.title('Correlation between Followers and Likes')\n",
        "  plt.xlabel('Number of Followers')\n",
        "  plt.ylabel('Number of Likes')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "NqX6A9_v1Bjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: User's Engagement Analysis"
      ],
      "metadata": {
        "id": "UZw7WGKS79B7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## User's Strength Score"
      ],
      "metadata": {
        "id": "p4rEk23ZhdJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def user_engagement_strength_score(user: User, usersEngagementData, number_results):\n",
        "  influence = calculate_influence(user, usersEngagementData, number_results)\n",
        "  reputation = calculate_reputation(user, usersEngagementData, number_results)\n",
        "\n",
        "  return influence * reputation"
      ],
      "metadata": {
        "id": "2OMMYIjegCys"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_egagement_data(user, key, usersEngagementData, number_results):\n",
        "  if usersEngagementData[user.id] is not None:\n",
        "    return usersEngagementData[user.id]\n",
        "  \n",
        "  if key == 'tweets':\n",
        "    tweets = api.get_timelines(user.id, max_results=number_results, tweet_fields=[\"attachments\",\"author_id\",\"context_annotations\",\"created_at\",\"entities\",\"geo\",\"in_reply_to_user_id\",\"lang\",\"public_metrics\",\"reply_settings\",\"source\"])\n",
        "    usersEngagementData[user.id][key] = tweets.data\n",
        "  elif key == 'replies':\n",
        "    mentions = api.search_tweets(query=f\"@{user.username}\", max_results=number_results)\n",
        "    usersEngagementData[user.id][key] = mentions.data\n",
        "  else:\n",
        "    replies = api.search_tweets(query=f\"to:{user.username}\", max_results=number_results)\n",
        "    usersEngagementData[user.id][key] = mentions.data"
      ],
      "metadata": {
        "id": "TBZ_om4XS7xc"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## User's Influence"
      ],
      "metadata": {
        "id": "4UfCkfrI8vK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_influence(user: User, usersEngagementData, number_results):\n",
        "    follower_count = user.public_metrics.followers_count\n",
        "\n",
        "    # Get user's tweet count and average engagement rate\n",
        "    engagement = get_user_egagement_data(user, 'tweets', usersEngagementData, number_results)\n",
        "    tweets = engagement['tweets']\n",
        "    tweet_count = len(tweets)\n",
        "    total_engagement = 0\n",
        "    \n",
        "    for tweet in tweets:\n",
        "        total_engagement += tweet.public_metrics.like_count + tweet.public_metrics.retweet_count + tweet.public_metrics.quote_count + tweet.public_metrics.reply_count\n",
        "        \n",
        "    if tweet_count > 0:\n",
        "        avg_engagement_rate = total_engagement / (tweet_count * follower_count) if total_engagement > 0 and tweet_count > 0 and follower_count > 0 else 0\n",
        "    else:\n",
        "        avg_engagement_rate = 0\n",
        "\n",
        "    # Calculate influence score\n",
        "    influence_score = math.log(follower_count + 1, 10) * (avg_engagement_rate + 1)\n",
        "    \n",
        "    return influence_score"
      ],
      "metadata": {
        "id": "bEHs1aAg8BU3"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## User's reputation"
      ],
      "metadata": {
        "id": "04dNyGZS9S3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_reputation(user: User, usersEngagementData, number_results):\n",
        "    # Get user's recent mentions and replies\n",
        "    engagementMentions = get_user_egagement_data(user, 'mentions', usersEngagementData, number_results)\n",
        "    engagementReplies = get_user_egagement_data(user, 'replies', usersEngagementData, number_results)\n",
        "\n",
        "    mentions = engagementMentions['mentions']\n",
        "    replies = engagementReplies['replies']\n",
        "\n",
        "    # Calculate reputation score based on sentiment analysis of mentions and replies\n",
        "    positive_sentiments = 0\n",
        "    negative_sentiments = 0\n",
        "    \n",
        "    for mention in mentions:\n",
        "        if mention.author_id != user.id:\n",
        "            sentiment = get_sentiment_score(mention.text)\n",
        "            if sentiment > 0:\n",
        "                positive_sentiments += 1\n",
        "            elif sentiment < 0:\n",
        "                negative_sentiments += 1\n",
        "                \n",
        "    for reply in replies:\n",
        "        if reply.author_id != user.id:\n",
        "            sentiment = get_sentiment_score(reply.text)\n",
        "            if sentiment > 0:\n",
        "                positive_sentiments += 1\n",
        "            elif sentiment < 0:\n",
        "                negative_sentiments += 1\n",
        "                \n",
        "    if (positive_sentiments + negative_sentiments) > 0:\n",
        "        \n",
        "        reputation_score = positive_sentiments / (positive_sentiments + negative_sentiments)\n",
        "    else:\n",
        "        reputation_score = 0\n",
        "        \n",
        "    fine_adjustment = 0.01\n",
        "    normalized_reputation_score = (reputation_score + user.public_metrics.listed_count * fine_adjustment) / (1 + (user.public_metrics.listed_count * fine_adjustment))\n",
        "\n",
        "    # Return influence and reputation scores\n",
        "    return normalized_reputation_score"
      ],
      "metadata": {
        "id": "jF8QN67-9XPb"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Tweet Metrics"
      ],
      "metadata": {
        "id": "BnUX2XgYMe7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_recency_score(created_at):\n",
        "    # Calculate the recency score of the tweet based on its age\n",
        "    # For example, using a logarithmic decay function\n",
        "\n",
        "    tweet_date = datetime.datetime.strptime(created_at, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
        "    now = datetime.datetime.now()\n",
        "\n",
        "    age_in_seconds = (now - tweet_date).total_seconds()\n",
        "    \n",
        "    if age_in_seconds < 0: \n",
        "       age_in_seconds = (tweet_date - now).total_seconds()\n",
        "\n",
        "    # Set the decay factor\n",
        "    decay_factor = 0.1\n",
        "\n",
        "    # Calculate the recency score using a logarithmic decay function\n",
        "    recency_score = 1 / (1 + decay_factor * math.log10(1 + age_in_seconds))\n",
        "\n",
        "    return recency_score"
      ],
      "metadata": {
        "id": "Wx-db4lnMhSl"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Social Capital Calculus"
      ],
      "metadata": {
        "id": "DVm73M3bJ_3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_social_capital_score(tweet: Tweet, usersById, usersByUserName):\n",
        "    # Extract relevant information from the tweet\n",
        "    text = tweet.text\n",
        "    attachments = tweet.attachments\n",
        "    public_metrics = tweet.public_metrics\n",
        "    created_at = tweet.created_at\n",
        "\n",
        "    # Preprocess tweet text\n",
        "    tokens = pre_processing_text(text)\n",
        "\n",
        "    # Calculate sentiment score\n",
        "    sentiment_score = get_sentiment_score(text)\n",
        "\n",
        "    # Calculate diversity score\n",
        "    diversity_score = calculate_diversity_score(tokens)\n",
        "\n",
        "    # Calculate recency score\n",
        "    recency_score = calculate_recency_score(created_at)\n",
        "\n",
        "    # Calculate context score\n",
        "    context_score = calculate_context_score(text, tokens)\n",
        "\n",
        "    # Calculate social capital score\n",
        "    likes = public_metrics.like_count\n",
        "    retweets = public_metrics.retweet_count\n",
        "    replies = public_metrics.reply_count\n",
        "    impressions_count = public_metrics.impression_count\n",
        "    num_hashtags = len(tweet.entities.hashtags)\n",
        "    num_urls = len(tweet.entities.urls)\n",
        "\n",
        "    mention_users_strenght_score = 0\n",
        "    if tweet.entities.mentions is not None:\n",
        "      for mention in tweet.entities.mentions:\n",
        "        if mention.username not in usersByUserName:\n",
        "          get_user_mentioned(mention.username)\n",
        "\n",
        "        mention_users_strenght_score += usersByUserName[mention.username]\n",
        "\n",
        "    num_media = 0\n",
        "    if attachments is not None and attachments.media_keys is not None:\n",
        "        num_media = len(attachments.media_keys)\n",
        "\n",
        "    #print(likes, retweets, replies, impressions_count, num_hashtags, num_urls, num_media, diversity_score, (influence_score * reputation_score), mention_users_strenght_score, len(tokens), context_score, recency_score)\n",
        "    \n",
        "    social_capital_score = ((usersById[tweet.author_id] + retweets + likes + replies + impressions_count + num_media + num_hashtags + num_urls + diversity_score + mention_users_strenght_score + len(tokens) + context_score) * recency_score)\n",
        "\n",
        "    return {'tweet': tweet, 'score': social_capital_score }"
      ],
      "metadata": {
        "id": "tKLBNqc8KEe5"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Build User Profile"
      ],
      "metadata": {
        "id": "jx13jwtEMBgq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Generate Recommendations"
      ],
      "metadata": {
        "id": "twzXlJFoMHzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_recommendations(subject, topN):\n",
        "  public_tweets, usersById, usersByUserName = collect_data(subject);\n",
        "  ranking = {}\n",
        "\n",
        "  for tweet in public_tweets.data:\n",
        "      social_capital_score = calculate_social_capital_score(tweet, usersById, usersByUserName)\n",
        "      ranking[tweet.id] = social_capital_score\n",
        "\n",
        "  ranked = dict(sorted(ranking.items(), key=lambda item: item[1]['score'], reverse=True)) \n",
        "\n",
        "  for tweetId in {k: ranked[k] for k in list(ranked)[:topN]}: \n",
        "      print(f\"SC: {ranking[tweetId]['score']} - TweetID: {ranking[tweetId]['tweet'].id}\")\n",
        "      print(ranking[tweetId]['tweet'].text)\n",
        "      print()"
      ],
      "metadata": {
        "id": "_b4K-GSJMQRf"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Defines the subject"
      ],
      "metadata": {
        "id": "dnlrND1zdeZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_recommendations(\"LuÃ­sa Sonza\", 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuRxYNdcdiwY",
        "outputId": "b154f367-500d-4ebb-9ded-f637471814b6"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SC: 71840.85671179024 - TweetID: 1634184303625007106\n",
            "LuÃ­sa Sonza Ã© a atraÃ§Ã£o da festa de amanhÃ£! #BBB23 https://t.co/X9fQ5x7nHc\n",
            "\n",
            "SC: 52157.552392563724 - TweetID: 1634509712027787265\n",
            "Bom dia minha aldeia, como vocÃªs estÃ£o? O Boss acordou todo mundo bem cedinho hoje, ja teve fila para o poder coringa e a prova do anjo acontece essa manhÃ£. Vamos torcer pra Domi ganhar e ver a famÃ­lia, ela merece muito! \n",
            "Mood de hoje pois tem fexxtinha com LuÃ­sa Sonza! ðŸ¥‚ #BBB23 https://t.co/sAeV6fSuwi\n",
            "\n",
            "SC: 36728.08020715328 - TweetID: 1634695238978600960\n",
            "Luisa Sonza falando com os brothers #BBB23 https://t.co/9yl75bLjEQ\n",
            "\n",
            "SC: 30086.113136239735 - TweetID: 1634696250875428865\n",
            "Luisa Sonza apareceu para dar um recado aos brothers e avisar que os figurinos jÃ¡ estÃ£o liberados!!!\n",
            "\n",
            "ReproduÃ§Ã£o: Globo/Globoplay\n",
            "\n",
            "#TeamAmanda #BBB23 https://t.co/wOmVeDDB19\n",
            "\n",
            "SC: 26343.3430293797 - TweetID: 1634738666076622848\n",
            "LuÃ­sa Sonza chegou para comandar a festa do #BBB23 ðŸ”¥ https://t.co/5mSxZEODxg\n",
            "\n"
          ]
        }
      ]
    }
  ]
}