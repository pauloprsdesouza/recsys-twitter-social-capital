{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+zvECdS+TJNnIYJQr1bD6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pauloprsdesouza/recsys-twitter-social-capital/blob/dev-colab/RecSysTwitterSocialCapital.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies"
      ],
      "metadata": {
        "id": "S_XUFOaMhF1e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJz0N5vzg2u2"
      },
      "outputs": [],
      "source": [
        "pip install emoji\n",
        "pip install python-twitter-v2\n",
        "pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import necessary libraries"
      ],
      "metadata": {
        "id": "GHzD8as6hMq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "import datetime\n",
        "import emoji\n",
        "import math\n",
        "from pytwitter.models import User, Tweet, TweetEntities, TweetEntitiesUrl, TweetPublicMetrics, TweetEntitiesMention, TweetEntitiesHashtag, TweetEntitiesAnnotation\n",
        "from pytwitter import Api\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "## Nltk Dependencies\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "stemmer = SnowballStemmer('english')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "## Bert Configuration\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "iWL3PuCYhRl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Api Bearer Definition"
      ],
      "metadata": {
        "id": "TdA_5dsNhor1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api = Api(bearer_token=\"AAAAAAAAAAAAAAAAAAAAAMhqlAEAAAAA4Pqzn354Z5nlkP5lKaW98vzlVlA%3D7GIA03xacVKdFYTFg7qmgvWTZThpa2FFd4SNPUqP7uPK7Xjue5\")"
      ],
      "metadata": {
        "id": "5478U2QbhsAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Collect Data"
      ],
      "metadata": {
        "id": "r_SIieVBhzVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "public_tweets = []\n",
        "\n",
        "def collect_data():\n",
        "    public_tweets = api.search_tweets(query=\"recommender systems lang:en has:hashtags -is:retweet has:media\", \n",
        "                                      expansions=[\"referenced_tweets.id.author_id\",\"in_reply_to_user_id\",\"attachments.media_keys\",\"author_id\",\"entities.mentions.username\"], \n",
        "                                      user_fields=[\"created_at\",\"entities\",\"id\",\"location\",\"name\",\"pinned_tweet_id\",\"profile_image_url\",\"protected\",\"public_metrics\",\"url\",\"username\",\"verified\"],\n",
        "                                      tweet_fields=[\"attachments\",\"author_id\",\"context_annotations\",\"created_at\",\"entities\",\"geo\",\"in_reply_to_user_id\",\"lang\",\"public_metrics\",\"reply_settings\",\"source\"], \n",
        "                                      max_results=50)"
      ],
      "metadata": {
        "id": "-pbQ5gKqh9DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setp 2: Pre-process Data"
      ],
      "metadata": {
        "id": "Q4aTV9stiGQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_processing():\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "\n",
        "    # Remove mentions and hashtags\n",
        "    text = re.sub(r\"@\\S+\", \"\", text)\n",
        "    text = re.sub\n",
        "\n",
        "    # Remove punctuation\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    tokens = [token for token in tokens if not token in stop_words]\n",
        "\n",
        "    # Lemmatize\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    for token in tokens:\n",
        "        lemma = lemmatizer.lemmatize(token, wordnet.VERB)\n",
        "        if lemma == token:\n",
        "            lemma = lemmatizer.lemmatize(token, wordnet.NOUN)\n",
        "        if lemma == token:\n",
        "            lemma = lemmatizer.lemmatize(token, wordnet.ADJ)\n",
        "        if lemma == token:\n",
        "            lemma = lemmatizer.lemmatize(token, wordnet.ADV)\n",
        "        lemmas.append\n",
        "\n",
        "    # Disambiguate polysemous words\n",
        "    tokens = disambiguate_polysemous_words(tokens)\n",
        "\n",
        "    # Expand synonyms\n",
        "    tokens = expand_synonyms(tokens)\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "eeOQuAF_iKhg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}