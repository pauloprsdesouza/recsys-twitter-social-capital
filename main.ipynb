{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytwitter import Api\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "import math\n",
    "from pytwitter.models import User;\n",
    "from pytwitter.models import Tweet;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = Api(bearer_token=\"AAAAAAAAAAAAAAAAAAAAAMhqlAEAAAAA4Pqzn354Z5nlkP5lKaW98vzlVlA%3D7GIA03xacVKdFYTFg7qmgvWTZThpa2FFd4SNPUqP7uPK7Xjue5\")\n",
    "\n",
    "public_tweets = api.search_tweets(query=\"allan dos santos lang:pt has:hashtags -is:retweet has:media\", expansions=[\"referenced_tweets.id.author_id\",\"in_reply_to_user_id\",\"attachments.media_keys\",\"author_id\",\"entities.mentions.username\"], \n",
    "                                  user_fields=[\"created_at\",\"entities\",\"id\",\"location\",\"name\",\"pinned_tweet_id\",\"profile_image_url\",\"protected\",\"public_metrics\",\"url\",\"username\",\"verified\"],\n",
    "                                  tweet_fields=[\"attachments\",\"author_id\",\"context_annotations\",\"created_at\",\"entities\",\"geo\",\"in_reply_to_user_id\",\"lang\",\"public_metrics\",\"reply_settings\",\"source\"], max_results=100, query_type='recent')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User's influence and reputation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_score(text):\n",
    "    \"float: The sentiment score between -1.0 (negative) and 1.0 (positive)\"\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence Calculus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_influence(user: User):\n",
    "    follower_count = user.public_metrics.followers_count\n",
    "\n",
    "    # Get user's tweet count and average engagement rate\n",
    "    tweets = api.get_timelines(user.id, max_results=50, tweet_fields=[\"attachments\",\"author_id\",\"context_annotations\",\"created_at\",\"entities\",\"geo\",\"in_reply_to_user_id\",\"lang\",\"public_metrics\",\"reply_settings\",\"source\"])\n",
    "    tweet_count = len(tweets.data)\n",
    "    total_engagement = 0\n",
    "    \n",
    "    for tweet in tweets.data:\n",
    "        total_engagement += tweet.public_metrics.like_count + tweet.public_metrics.retweet_count + tweet.public_metrics.quote_count + tweet.public_metrics.reply_count\n",
    "        \n",
    "    if tweet_count > 0:\n",
    "        avg_engagement_rate = total_engagement / (tweet_count * follower_count) if total_engagement > 0 and tweet_count > 0 and follower_count > 0 else 0\n",
    "    else:\n",
    "        avg_engagement_rate = 0\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate influence score\n",
    "    influence_score = math.log(follower_count + 1, 10) * (avg_engagement_rate + 1)\n",
    "    \n",
    "    return influence_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reputation Calculus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reputation(user: User):\n",
    "    # Get user's recent mentions and replies\n",
    "    mentions = api.search_tweets(query=f\"@{user.username}\", max_results=50)\n",
    "    replies = api.search_tweets(query=f\"to:{user.username}\", max_results=50)\n",
    "\n",
    "    # Calculate reputation score based on sentiment analysis of mentions and replies\n",
    "    positive_sentiments = 0\n",
    "    negative_sentiments = 0\n",
    "    \n",
    "    for mention in mentions.data:\n",
    "        if mention.author_id != user.id:\n",
    "            sentiment = get_sentiment_score(mention.text)\n",
    "            if sentiment > 0:\n",
    "                positive_sentiments += 1\n",
    "            elif sentiment < 0:\n",
    "                \n",
    "                negative_sentiments += 1\n",
    "                \n",
    "    for reply in replies.data:\n",
    "        if reply.author_id != user.id:\n",
    "            sentiment = get_sentiment_score(reply.text)\n",
    "            if sentiment > 0:\n",
    "                positive_sentiments += 1\n",
    "            elif sentiment < 0:\n",
    "                negative_sentiments += 1\n",
    "                \n",
    "    if (positive_sentiments + negative_sentiments) > 0:\n",
    "        \n",
    "        reputation_score = positive_sentiments / (positive_sentiments + negative_sentiments)\n",
    "    else:\n",
    "        reputation_score = 0\n",
    "        \n",
    "    fine_adjustment = 0.01\n",
    "    normalized_reputation_score = (reputation_score + user.public_metrics.listed_count * fine_adjustment) / (1 + (user.public_metrics.listed_count * fine_adjustment))\n",
    "\n",
    "    # Return influence and reputation scores\n",
    "    return normalized_reputation_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Paulo\n",
      "[nltk_data]     Roberto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Paulo\n",
      "[nltk_data]     Roberto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Paulo\n",
      "[nltk_data]     Roberto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw to C:\\Users\\Paulo\n",
      "[nltk_data]     Roberto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Paulo Roberto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "stemmer = SnowballStemmer('portuguese')\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "def remove_urls(text):\n",
    "    return re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "def remove_mentions(text):\n",
    "    return re.sub(r\"@\\S+\", \"\", text)\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    return re.sub(r\"#\\S+\", \"\", text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text, language='portuguese')\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    return [token for token in tokens if not token in stop_words]\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for token in tokens:\n",
    "        lemma = lemmatizer.lemmatize(token, wordnet.VERB)\n",
    "        if lemma == token:\n",
    "            lemma = lemmatizer.lemmatize(token, wordnet.NOUN)\n",
    "        if lemma == token:\n",
    "            lemma = lemmatizer.lemmatize(token, wordnet.ADJ)\n",
    "        if lemma == token:\n",
    "            lemma = lemmatizer.lemmatize(token, wordnet.ADV)\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def synonymize(tokens):\n",
    "    synonyms = []\n",
    "    for token in tokens:\n",
    "        synsets = wordnet.synsets(token, lang='por')\n",
    "        if synsets:\n",
    "            synset = synsets[0]\n",
    "            for lemma in synset.lemmas(lang='por'):\n",
    "                synonym = lemma.name().lower()\n",
    "                if synonym not in synonyms and synonym != token:\n",
    "                    synonyms.append(synonym)\n",
    "    return synonyms\n",
    "\n",
    "def polysemmize(tokens):\n",
    "    for word in tokens:\n",
    "       pos = nltk.pos_tag(tokens)[0][1][0].lower()\n",
    "       if pos not in ['n', 'v']:\n",
    "           continue\n",
    "        # Use simple_lesk to disambiguate the sense of the word\n",
    "       synset = nltk.wsd.lesk(tokens, word, pos=pos)\n",
    "    \n",
    "       if synset is not None:\n",
    "           # Replace the token with the lemma of the most likely sense\n",
    "           word = synset.lemmas()[0].name().lower()\n",
    "            \n",
    "    return tokens\n",
    "\n",
    "def preprocess_tweet(text):\n",
    "    text = text.lower()\n",
    "    text = remove_urls(text)\n",
    "    text = remove_mentions(text)\n",
    "    text = remove_hashtags(text)\n",
    "    text = remove_punctuation(text)\n",
    "    tokens = tokenize(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = lemmatize(tokens)\n",
    "    tokens = polysemmize(tokens)\n",
    "    tokens.extend(synonymize(tokens))\n",
    "    return set(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Reputation and Influence for all users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "usersByUsername = {}\n",
    "usersById = {}\n",
    "for user in public_tweets.includes.users:\n",
    "    result = [calculate_influence(user), calculate_reputation(user)]\n",
    "    usersByUsername[user.username] = result\n",
    "    usersById[user.id] = result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculates the social capital from a tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import emoji\n",
    "\n",
    "def calculate_social_capital(tweet: Tweet):\n",
    "    # Extract relevant information from the tweet\n",
    "    tokens = preprocess_tweet(tweet.text)\n",
    "    urls = re.findall('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', tweet.text)\n",
    "    emojis = emoji.emoji_count(tweet.text)\n",
    "    likes = tweet.public_metrics.like_count\n",
    "    retweets = tweet.public_metrics.retweet_count\n",
    "    replies = tweet.public_metrics.reply_count\n",
    "    quotes = tweet.public_metrics.quote_count\n",
    "    created_at = tweet.created_at\n",
    "    hashtags = len(re.findall(r'#(\\w+)', tweet.text))\n",
    "    \n",
    "    num_medias = 0\n",
    "    if tweet.attachments is not None and tweet.attachments.media_keys is not None:\n",
    "        for attachment in tweet.attachments.media_keys:\n",
    "            num_medias += 1\n",
    "\n",
    "    # Calculate the length of the tweet in characters\n",
    "    length = len(tweet.text)\n",
    "\n",
    "    # Calculate the sentiment score of the tweet\n",
    "    sentiment_score = get_sentiment_score(tweet.text)\n",
    "\n",
    "    # Calculate the diversity score of the tweet\n",
    "    diversity_score = calculate_diversity_score(tokens)\n",
    "\n",
    "    # Calculate the number of resources in the tweet\n",
    "    num_resources = len(urls) + emojis + num_medias\n",
    "    \n",
    "    \n",
    "\n",
    "    # Calculate the recency score of the tweet\n",
    "    recency_score = calculate_recency_score(created_at)\n",
    "\n",
    "    # Calculate the social capital score of the tweet\n",
    "    engagement = (likes + replies + quotes)\n",
    "    social_capital_score = (retweets if retweets > 0 else 1) * ((engagement + num_resources + diversity_score + len(tokens) + hashtags + length + sum(usersById[tweet.author_id])) * recency_score)\n",
    "\n",
    "    return {'tweet': tweet, 'score': social_capital_score } \n",
    "\n",
    "def get_sentiment_score(text):\n",
    "    # Use a sentiment analysis library or model to calculate the sentiment score of the tweet text\n",
    "    # For example, using TextBlob library\n",
    "    from textblob import TextBlob\n",
    "\n",
    "    blob = TextBlob(text)\n",
    "    sentiment_score = blob.sentiment.polarity\n",
    "\n",
    "    return sentiment_score\n",
    "\n",
    "def calculate_diversity_score(words):\n",
    "    # Use a measure of lexical diversi\n",
    "    # ty to calculate the diversity score of the tweet text\n",
    "    # For example, using the type-token ratio (TTR) metric\n",
    "    num_words = len(words)\n",
    "    num_unique_words = len(set(words))\n",
    "    diversity_score = num_unique_words / num_words\n",
    "\n",
    "\n",
    "\n",
    "    return diversity_score\n",
    "\n",
    "def calculate_recency_scoreV2(created_at):\n",
    "    # Calculate the recency score of the tweet based on its age\n",
    "    # For example, using a linear decay function with a half-life of one day\n",
    "    tweet_date = datetime.datetime.strptime(created_at, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    age_in_seconds = (now - tweet_date).total_seconds()\n",
    "    half_life_in_seconds = 86400 # One day in seconds\n",
    "    \n",
    "    recency_score = 0.5 ** (age_in_seconds / half_life_in_seconds)\n",
    "\n",
    "    return recency_score\n",
    "\n",
    "def calculate_recency_score(created_at):\n",
    "    # Calculate the recency score of the tweet based on its age\n",
    "    # For example, using a logarithmic decay function\n",
    "    \n",
    "    tweet_date = datetime.datetime.strptime(created_at, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    age_in_seconds = (now - tweet_date).total_seconds()\n",
    "    \n",
    "    if age_in_seconds < 0: \n",
    "        age_in_seconds = (tweet_date - now).total_seconds()\n",
    "    \n",
    "    # Set the decay factor\n",
    "    decay_factor = 0.1\n",
    "    \n",
    "    # Calculate the recency score using a logarithmic decay function\n",
    "    recency_score = 1 / (1 + decay_factor * math.log(1 + age_in_seconds, 10))\n",
    "    \n",
    "    return recency_score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "\n",
    "def get_tweet_social_capital(tweet: Tweet):\n",
    "    # Get tweet text\n",
    "    text = tweet.text\n",
    "    \n",
    "    # Get number of likes and retweets\n",
    "    likes = tweet.public_metrics.like_count\n",
    "    retweets = tweet.public_metrics.retweet_count\n",
    "    replies = tweet.public_metrics.reply_count\n",
    "    quotes = tweet.public_metrics.quote_count\n",
    "    hashtags = len(re.findall(r'#(\\w+)', text))\n",
    "    words = len(preprocess_tweet(text))\n",
    "    \n",
    "    words = words*5 if hashtags > words else words\n",
    "    \n",
    "    # Get tweet creation time\n",
    "    created_at = tweet.created_at\n",
    "    created_at = datetime.datetime.strptime(created_at, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    now = datetime.datetime.utcnow()\n",
    "    age = (now - created_at).total_seconds() / 3600 # tweet age in hours\n",
    "    \n",
    "    # Calculate recency score\n",
    "    recency_score = math.exp(-0.1 * age)\n",
    "    \n",
    "    # Get URLs in tweet\n",
    "    urls = re.findall('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', text)\n",
    "    num_urls = len(urls)\n",
    "    \n",
    "    # Get number of emojis in tweet\n",
    "    emojis = re.findall(r'[^\\w\\s,]', text)\n",
    "    num_emojis = len(emojis)\n",
    "    \n",
    "    # Get number of photos and videos in tweet\n",
    "    num_medias = 0\n",
    "    \n",
    "    if tweet.attachments is not None and tweet.attachments.media_keys is not None:\n",
    "        for attachment in tweet.attachments.media_keys:\n",
    "            num_medias += 1\n",
    "            \n",
    "    # mention_score = 0\n",
    "    # if tweet.entities.mentions is not None:\n",
    "    #     for mention in tweet.entities.mentions:\n",
    "    #         mention_score += sum(usersByUsername[mention.username])\n",
    "    \n",
    "    # Calculate media score\n",
    "    media_score = 0.2 * num_medias\n",
    "    \n",
    "    # Calculate URL score\n",
    "    url_score = 0.2 * num_urls\n",
    "    \n",
    "    # Calculate emoji score\n",
    "    emoji_score = 0.5 * num_emojis\n",
    "    \n",
    "    # Calculate engagement score\n",
    "    engagement_score = likes + retweets + replies + quotes\n",
    "    \n",
    "    # Calculate social capital\n",
    "    social_capital = recency_score * (media_score + url_score + emoji_score + engagement_score + hashtags + words + sum(usersById[tweet.author_id]))\n",
    "    \n",
    "    return {'tweet': tweet, 'score': social_capital }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking tweets list from social capital score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[210], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m ranking \u001b[39m=\u001b[39m {}\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m tweet \u001b[39min\u001b[39;00m public_tweets\u001b[39m.\u001b[39mdata:          \n\u001b[1;32m----> 3\u001b[0m     ranking[tweet\u001b[39m.\u001b[39mid] \u001b[39m=\u001b[39m calculate_social_capital(tweet)\n\u001b[0;32m      5\u001b[0m ranked \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39msorted\u001b[39m(ranking\u001b[39m.\u001b[39mitems(), key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m item: item[\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m], reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m tweetId \u001b[39min\u001b[39;00m ranked: \n",
      "Cell \u001b[1;32mIn[209], line 36\u001b[0m, in \u001b[0;36mcalculate_social_capital\u001b[1;34m(tweet)\u001b[0m\n\u001b[0;32m     31\u001b[0m num_resources \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(urls) \u001b[39m+\u001b[39m emojis \u001b[39m+\u001b[39m num_medias\n\u001b[0;32m     35\u001b[0m \u001b[39m# Calculate the recency score of the tweet\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m recency_score \u001b[39m=\u001b[39m calculate_recency_score(created_at)\n\u001b[0;32m     38\u001b[0m \u001b[39m# Calculate the social capital score of the tweet\u001b[39;00m\n\u001b[0;32m     39\u001b[0m engagement \u001b[39m=\u001b[39m (likes \u001b[39m+\u001b[39m replies \u001b[39m+\u001b[39m quotes)\n",
      "Cell \u001b[1;32mIn[209], line 92\u001b[0m, in \u001b[0;36mcalculate_recency_score\u001b[1;34m(created_at)\u001b[0m\n\u001b[0;32m     89\u001b[0m     age_in_seconds \u001b[39m=\u001b[39m (tweet_date \u001b[39m-\u001b[39m now)\u001b[39m.\u001b[39mtotal_seconds()\n\u001b[0;32m     91\u001b[0m \u001b[39m# Set the decay factor\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m decay_factor \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m\n\u001b[0;32m     94\u001b[0m \u001b[39m# Calculate the recency score using a logarithmic decay function\u001b[39;00m\n\u001b[0;32m     95\u001b[0m recency_score \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m decay_factor \u001b[39m*\u001b[39m math\u001b[39m.\u001b[39mlog(\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m age_in_seconds, \u001b[39m10\u001b[39m))\n",
      "Cell \u001b[1;32mIn[209], line 92\u001b[0m, in \u001b[0;36mcalculate_recency_score\u001b[1;34m(created_at)\u001b[0m\n\u001b[0;32m     89\u001b[0m     age_in_seconds \u001b[39m=\u001b[39m (tweet_date \u001b[39m-\u001b[39m now)\u001b[39m.\u001b[39mtotal_seconds()\n\u001b[0;32m     91\u001b[0m \u001b[39m# Set the decay factor\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m decay_factor \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m\n\u001b[0;32m     94\u001b[0m \u001b[39m# Calculate the recency score using a logarithmic decay function\u001b[39;00m\n\u001b[0;32m     95\u001b[0m recency_score \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m decay_factor \u001b[39m*\u001b[39m math\u001b[39m.\u001b[39mlog(\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m age_in_seconds, \u001b[39m10\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_frame.py:988\u001b[0m, in \u001b[0;36mPyDBFrame.trace_dispatch\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m    986\u001b[0m \u001b[39m# if thread has a suspend flag, we suspend with a busy wait\u001b[39;00m\n\u001b[0;32m    987\u001b[0m \u001b[39mif\u001b[39;00m info\u001b[39m.\u001b[39mpydev_state \u001b[39m==\u001b[39m STATE_SUSPEND:\n\u001b[1;32m--> 988\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_wait_suspend(thread, frame, event, arg)\n\u001b[0;32m    989\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrace_dispatch\n\u001b[0;32m    990\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_frame.py:165\u001b[0m, in \u001b[0;36mPyDBFrame.do_wait_suspend\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdo_wait_suspend\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 165\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_args[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mdo_wait_suspend(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.01\u001b[39m)\n\u001b[0;32m   2108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ranking = {}\n",
    "for tweet in public_tweets.data:          \n",
    "    ranking[tweet.id] = calculate_social_capital(tweet)\n",
    "    \n",
    "ranked = dict(sorted(ranking.items(), key=lambda item: item[1]['score'], reverse=True))\n",
    "\n",
    "for tweetId in ranked: \n",
    "    print(ranking[tweetId]['score'], ranking[tweetId]['tweet'].id + \"    ----  \" + ranking[tweetId]['tweet'].text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence vs Interactions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reputation vs Interactions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram Interactions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0ab0a4bd07e653e451a64cb3a171ddec94ddedb71f86f0f21941dd76a8744c36"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
