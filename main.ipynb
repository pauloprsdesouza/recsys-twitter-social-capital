{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytwitter import Api\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "import math\n",
    "from pytwitter.models import User;\n",
    "from pytwitter.models import Tweet;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = Api(bearer_token=\"AAAAAAAAAAAAAAAAAAAAAMhqlAEAAAAA4Pqzn354Z5nlkP5lKaW98vzlVlA%3D7GIA03xacVKdFYTFg7qmgvWTZThpa2FFd4SNPUqP7uPK7Xjue5\")\n",
    "\n",
    "public_tweets = api.search_tweets(query=\"bolsonaro lang:pt has:hashtags -is:retweet\", expansions=[\"referenced_tweets.id.author_id\",\"in_reply_to_user_id\",\"attachments.media_keys\",\"author_id\",\"entities.mentions.username\"], \n",
    "                                  user_fields=[\"created_at\",\"entities\",\"id\",\"location\",\"name\",\"pinned_tweet_id\",\"profile_image_url\",\"protected\",\"public_metrics\",\"url\",\"username\",\"verified\"],\n",
    "                                  tweet_fields=[\"attachments\",\"author_id\",\"context_annotations\",\"created_at\",\"entities\",\"geo\",\"in_reply_to_user_id\",\"lang\",\"public_metrics\",\"reply_settings\",\"source\"], max_results=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User's influence and reputation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_score(text):\n",
    "    \"float: The sentiment score between -1.0 (negative) and 1.0 (positive)\"\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence Calculus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_influence(user: User):\n",
    "    follower_count = user.public_metrics.followers_count\n",
    "\n",
    "    # Get user's tweet count and average engagement rate\n",
    "    tweets = api.get_timelines(user.id, max_results=100, tweet_fields=[\"attachments\",\"author_id\",\"context_annotations\",\"created_at\",\"entities\",\"geo\",\"in_reply_to_user_id\",\"lang\",\"public_metrics\",\"reply_settings\",\"source\"])\n",
    "    tweet_count = len(tweets.data)\n",
    "    total_engagement = 0\n",
    "    \n",
    "    for tweet in tweets.data:\n",
    "        total_engagement += tweet.public_metrics.like_count + tweet.public_metrics.retweet_count + tweet.public_metrics.quote_count + tweet.public_metrics.reply_count\n",
    "        \n",
    "    if tweet_count > 0:\n",
    "        avg_engagement_rate = total_engagement / (tweet_count * follower_count) if total_engagement > 0 and tweet_count > 0 and follower_count > 0 else 0\n",
    "    else:\n",
    "        avg_engagement_rate = 0\n",
    "\n",
    "    # Calculate influence score\n",
    "    influence_score = math.log(follower_count + 1, 10) * (avg_engagement_rate + 1)\n",
    "    \n",
    "    return influence_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reputation Calculus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reputation(user: User):\n",
    "    # Get user's recent mentions and replies\n",
    "    mentions = api.search_tweets(query=f\"@{user.username}\", max_results=100)\n",
    "    replies = api.search_tweets(query=f\"to:{user.username}\", max_results=100)\n",
    "\n",
    "    # Calculate reputation score based on sentiment analysis of mentions and replies\n",
    "    positive_sentiments = 0\n",
    "    negative_sentiments = 0\n",
    "    \n",
    "    for mention in mentions.data:\n",
    "        if mention.author_id != user.id:\n",
    "            sentiment = get_sentiment_score(mention.text)\n",
    "            if sentiment > 0:\n",
    "                positive_sentiments += 1\n",
    "            elif sentiment < 0:\n",
    "                negative_sentiments += 1\n",
    "                \n",
    "    for reply in replies.data:\n",
    "        if reply.author_id != user.id:\n",
    "            sentiment = get_sentiment_score(reply.text)\n",
    "            if sentiment > 0:\n",
    "                positive_sentiments += 1\n",
    "            elif sentiment < 0:\n",
    "                negative_sentiments += 1\n",
    "                \n",
    "    if (positive_sentiments + negative_sentiments) > 0:\n",
    "        reputation_score = positive_sentiments / (positive_sentiments + negative_sentiments)\n",
    "    else:\n",
    "        reputation_score = 0\n",
    "\n",
    "    # Return influence and reputation scores\n",
    "    return reputation_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def remove_urls(text):\n",
    "    \"\"\"Remove URLs from a text\"\"\"\n",
    "    return re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "def remove_mentions(text):\n",
    "    \"\"\"Remove mentions from a text\"\"\"\n",
    "    return re.sub(r\"@\\S+\", \"\", text)\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    \"\"\"Remove hashtags from a text\"\"\"\n",
    "    return re.sub(r\"#\\S+\", \"\", text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"Remove punctuation from a text\"\"\"\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Tokenize a text\"\"\"\n",
    "    return word_tokenize(text, language='portuguese')\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Remove stopwords from a list of tokens\"\"\"\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    return [token for token in tokens if not token in stop_words]\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    \"\"\"Lemmatize a list of tokens\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for token in tokens:\n",
    "        lemma = lemmatizer.lemmatize(token, wordnet.VERB)\n",
    "        if lemma == token:\n",
    "            lemma = lemmatizer.lemmatize(token, wordnet.NOUN)\n",
    "        if lemma == token:\n",
    "            lemma = lemmatizer.lemmatize(token, wordnet.ADJ)\n",
    "        if lemma == token:\n",
    "            lemma = lemmatizer.lemmatize(token, wordnet.ADV)\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def synonymize(tokens):\n",
    "    \"\"\"Synonymize a list of tokens\"\"\"\n",
    "    synonyms = []\n",
    "    for token in tokens:\n",
    "        synsets = wordnet.synsets(token, lang='por')\n",
    "        if synsets:\n",
    "            synset = synsets[0]\n",
    "            for lemma in synset.lemmas(lang='por'):\n",
    "                synonym = lemma.name()\n",
    "                if synonym not in synonyms and synonym != token:\n",
    "                    synonyms.append(synonym)\n",
    "    return synonyms\n",
    "\n",
    "def preprocess_tweet(text):\n",
    "    \"\"\"Preprocess a Brazilian Portuguese tweet\"\"\"\n",
    "    text = text.lower()\n",
    "    text = remove_urls(text)\n",
    "    text = remove_mentions(text)\n",
    "    text = remove_hashtags(text)\n",
    "    text = remove_punctuation(text)\n",
    "    tokens = tokenize(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = lemmatize(tokens)\n",
    "    tokens.extend(synonymize(tokens))\n",
    "    return tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Social Capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "\n",
    "def get_tweet_social_capital(tweet: Tweet):\n",
    "    # Get tweet text\n",
    "    text = tweet.text\n",
    "    \n",
    "    # Get number of likes and retweets\n",
    "    likes = tweet.public_metrics.like_count\n",
    "    retweets = tweet.public_metrics.retweet_count\n",
    "    replies = tweet.public_metrics.reply_count\n",
    "    words = len(text.split())\n",
    "    hashtags = len(re.findall(r'#(\\w+)', text))\n",
    "    \n",
    "    # Get tweet creation time\n",
    "    created_at = tweet.created_at\n",
    "    created_at = datetime.datetime.strptime(created_at, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    now = datetime.datetime.utcnow()\n",
    "    age = (now - created_at).total_seconds() / 3600 # tweet age in hours\n",
    "    \n",
    "    # Calculate recency score\n",
    "    recency_score = math.exp(-0.1 * age)\n",
    "    \n",
    "    # Get URLs in tweet\n",
    "    urls = re.findall('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', text)\n",
    "    num_urls = len(urls)\n",
    "    \n",
    "    # Get number of emojis in tweet\n",
    "    emojis = re.findall(r'[^\\w\\s,]', text)\n",
    "    num_emojis = len(emojis)\n",
    "    \n",
    "    # Get number of photos and videos in tweet\n",
    "    num_medias = 0\n",
    "    \n",
    "    if tweet.attachments is not None and tweet.attachments.media_keys is not None:\n",
    "        for attachment in tweet.attachments.media_keys:\n",
    "            num_medias += 1\n",
    "    \n",
    "    # Calculate media score\n",
    "    media_score = 0.5 * num_medias\n",
    "    \n",
    "    # Calculate URL score\n",
    "    url_score = 0.5 * num_urls\n",
    "    \n",
    "    # Calculate emoji score\n",
    "    emoji_score = 0.5 * num_emojis\n",
    "    \n",
    "    # Calculate engagement score\n",
    "    engagement_score = likes + retweets + replies\n",
    "    \n",
    "    # Calculate social capital\n",
    "    social_capital = recency_score * (media_score + url_score + emoji_score + engagement_score + words + hashtags)\n",
    "    \n",
    "    return {'tweet': tweet, 'score': social_capital }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Capital Calculus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = {}\n",
    "for user in public_tweets.includes.users:\n",
    "    teste[user.id] = [calculate_influence(user), calculate_reputation(user)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'le40747700': 24.753100092618098, 'lucineianiro57': 6.31146906070576, 'maykonwilian2': 6.279055666699698, 'jornalistavitor': 5.278702875974146, 'GuimaraesLiih': 4.811849016425214, 'betobraga14': 4.2705576070370315, 'Civuca5': 4.1008624434477765, 'flordelicadezas': 3.8037166841769916, 'DiCavaletti': 3.4505540974923665, 'pripstuita': 3.3978202526278927, 'guimarques1985': 3.3658975931999717, 'JaceNordestino': 3.23587311613621, 'DesculpasDoPT': 3.122870922864435, 'PoliticaBR5': 3.0935918820472077, 'Xavierhoz07': 2.0614270026106114, 'garcia15_stela': 1.3507329164653912, 'TaniaDur1': 1.0604019886144498, 'JoseAiltonGLim2': 0.7024648543576988, 'Vandersonjrs': 0.0}\n"
     ]
    }
   ],
   "source": [
    "ranking = {}\n",
    "usersTeste = {}\n",
    "for tweet in public_tweets.data:\n",
    "    for bla in public_tweets.includes.users:\n",
    "        if(bla.id == tweet.author_id):\n",
    "            usersTeste[bla.username] = sum(teste[tweet.author_id])\n",
    "            \n",
    "    ranking[tweet.id] = get_tweet_social_capital(tweet)\n",
    "    ranking[tweet.id]['score'] += sum(teste[tweet.author_id]) \n",
    "    \n",
    "    \n",
    "rankedUsers = dict(sorted(usersTeste.items(), key=lambda item: item[1], reverse=True))\n",
    "print(rankedUsers)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.31735854610308 O bandido do Bolsonaro, depois de assassinar 7 pessoas, dentre elas uma criança de 12 anos que corria para salvar sua vida, atirou para matar policiais. Quem já sabia que essa gente armada iria se voltar contra as forças de segurança? Satisfeitos? #policiamilitar #exercito https://t.co/trAW2KD5Ym\n",
      "53.75598989219125 O Palácio do Planalto busca aliados dentro dos partidos que estiveram ao lado do ex-presidente Jair Bolsonaro (PL) na eleição do ano passado.\n",
      "\n",
      "O objetivo é ampliar a base no congresso, mesmo sem contar com o apoio formal dessas legendas. \n",
      "#JornalDaCultura #JC https://t.co/mz2gHncAFc\n",
      "52.96137450697559 crime da sinuca em #Sinop: Os bolsominions q assassinaram 7 pessoas após perder na sinuca tinham passagens por roubo, violência doméstica e porte ilegal de armas.\n",
      "Mesmo assim o governo Bolsonaro deu ARMAS LEGALIZADAS aos 2 criminosos.#BrasilUrgente #bolsonaro #CidadeAlerta #BBB23 https://t.co/bkUeuA0HND\n",
      "48.2397036236754 Repeteco mas atual:\n",
      "De 2019 a 2022, sob o (des)governo de jair messias bolsonaro, vivemos um tipo de Robin Hood às avessas: Privaram o pobre e abastaram os ricos. Eis a foto do governo do mito. \n",
      "\n",
      "#canalhas \n",
      "#Hipocritas \n",
      "#fora\n",
      "47.97377007322257 #Brasil #Política Julgador que desrespeita CF não merece respeito nenhum!!!! É mais que palhaçada. Indulto de Temer vale e não interessa razões. Indulto de Bolsonaro prende indultado e aí é diferente? Não dá. Não é jurídico e da raiva.\n",
      "46.42782165281177 @cacabylima @AdriianaGaucha E a facada em Bolsonaro? E o 08de janeiro? #CPMI08dejaneiro #DanielSilveiraLivre #caciquetsererelivre #andersontorreslivre\n",
      "44.499078482128745 Chico Pinheiro desce a lenha em Bolsonaro, relembra tragédia, cita Lula e gera polêmica na web\n",
      "\n",
      "Fique por dentro das notícias! Siga @PoliticaBR5. Atualizações diárias.\n",
      "\n",
      "#politica #politico #politicas...\n",
      "\n",
      " Link da notícia: https://t.co/tZUN520WEU\n",
      "44.25837693162325 \"Essa tendência governista é ruim porque não cria plataformas, discussões. Um debate sobre propostas\" Para o #JornaldaCultura, @gesner_oliveira\n",
      " comenta busca do governo Lula em ter apoio de partidos aliados ao ex-presidente Jair Bolsonaro. https://t.co/rkr6pvZ9QQ\n",
      "43.3795677879013 @joaodamoedo Vc nem sabe oque está falando.\n",
      "Sou de SINOP-MT e tinha nenhum #CAC ENVOLVIDO..  ambos teria posso de arma bem antes de bolsonaro segundo seu próprio registro... e foi dado pelo @exercitooficial\n",
      "39.12694678494249 E assim mais uma vez Bandidos travestidos de \"cidadãos de bem\", \"patriotas\", \"cristãos\", \"pró-vida\", armamentista, CAC causaram mais uma chacina tenho o Bolsonaro assassino como co-responsavel.\n",
      "\n",
      "#BolsonaroPreso https://t.co/BH0AyLFbnB\n",
      "36.628132718700556 @pedroponciobr Quando Bolsonaro não foi na Bahia alagada e mandou seus ministros , quase esfolaram o homem vivo e agora? Por que estão no Carnaval ao invés de estarem ajudando o povo #ForaLula\n",
      "35.97637668130991 @exercitooficial @jairbolsonaro  Se os generais ameaçaram de prender Bolsonaro, e tais Generais entregou o Brasil, os Senhores ainda não os prendeu por que?!Confira o vídeo de RIBEIRO! #TikTok https://t.co/gxprquWjMe\n",
      "33.86823637526187 @nikolas_dm Primeira dama desqualificada. Alheia ao sofrimento do povo. É tudo que o brasileiro não precisava. Governo incompetente e com ministérios liderados por corruptos. Quanta diferença do governo Bolsonaro. #michellepresidente\n",
      "30.522169218277316 Eu amo Olavo de Carvalho. \n",
      "Eu amo Bolsonaro.\n",
      "Amo o Brasil.\n",
      "#Olavo https://t.co/Xgobp2WYiN\n",
      "26.365819116141402 Foram 14 anos de roubalheira no governo mas os desgraçados só falam do Bolsonaro. Bando de fdps apoiadores de ladrão. #LulaGenocida\n",
      "23.207302169914616 Se o Guime colocar um do casal Bolsonaro eu fico 2 dias sem falar mal dele #BBB23\n",
      "22.476882391231932 #Meu Presidente Bolsonaro E Do Brasil 🇧🇷 Não Reconheço Outro https://t.co/skCT20Lr1N\n",
      "20.29009657572731 Black sendo o capitão do mato do #BBB23\n",
      "Servindo o casal bolsonaro GusKey.\n",
      "17.4323889424434 As delações premiadas são um golpe do Bolsonaro para subverter os pobres! #DesculpasDoPT\n",
      "15.778117004564324 @siteptbr Vão dizer que foi culpa da gestão do #bolsonaro mozo.. pra vocês!\n"
     ]
    }
   ],
   "source": [
    "ranked = dict(sorted(ranking.items(), key=lambda item: item[1]['score'], reverse=True))\n",
    "for tweet in ranked: \n",
    "    print(ranking[tweet]['score'], ranking[tweet]['tweet'].text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence vs Interactions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reputation vs Interactions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram Interactions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_social_capital(tweets):\n",
    "    public_tweets = api.search_tweets(query=\"stf lang:pt has:hashtags is:retweet\", expansions=[\"referenced_tweets.id.author_id\",\"in_reply_to_user_id\",\"attachments.media_keys\",\"author_id\",\"entities.mentions.username\"],\n",
    "    user_fields=[\"created_at\",\"entities\",\"id\",\"location\",\"name\",\"pinned_tweet_id\",\"profile_image_url\",\"protected\",\"public_metrics\",\"url\",\"username\",\"verified\"],\n",
    "    tweet_fields=[\"attachments\",\"author_id\",\"context_annotations\",\"created_at\",\"entities\",\"geo\",\"in_reply_to_user_id\",\"lang\",\"public_metrics\",\"reply_settings\",\"source\"], max_results=100)\n",
    "\n",
    "    api.get_followers()\n",
    "\n",
    "    public_tweets.includes.tweets\n",
    "\n",
    "#analyze_social_capital(public_tweets.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def social_capital_impact(tweet, social_connections, interactions, texts, resources, user_popularity, user_influence):\n",
    "  # extract the user, mentions, and hashtags from the tweet\n",
    "  user = tweet['user']\n",
    "  mentions = tweet['mentions']\n",
    "  hashtags = tweet['hashtags']\n",
    "  \n",
    "  # compute the number of connections the user has in the social network\n",
    "  user_connections = np.sum(social_connections[user])\n",
    "  \n",
    "  # compute the number of connections between the user and the mentions\n",
    "  mention_connections = 0\n",
    "  for mention in mentions:\n",
    "    mention_connections += social_connections[user][mention]\n",
    "  \n",
    "  # compute the number of connections between the user and the hashtags\n",
    "  hashtag_connections = 0\n",
    "  for hashtag in hashtags:\n",
    "    hashtag_connections += social_connections[user][hashtag]\n",
    "  \n",
    "  # compute the sum of the resources associated with the tweet\n",
    "  resource_sum = 0\n",
    "  for resource in resources:\n",
    "    if resource in tweet['resources']:\n",
    "      resource_sum += resources[resource]\n",
    "  \n",
    "  # compute the number of interactions for the tweet\n",
    "  interaction_sum = interactions[tweet['tweet_id']]\n",
    "  \n",
    "  # compute the text similarity between the tweet and the other texts\n",
    "  vectorizer = TfidfVectorizer()\n",
    "  tweet_text = [tweet['text']]\n",
    "  tweet_vector = vectorizer.fit_transform(tweet_text)\n",
    "  other_vectors = vectorizer.transform(texts)\n",
    "  similarity = cosine_similarity(tweet_vector, other_vectors)\n",
    "  \n",
    "  # compute the user popularity and influence factors\n",
    "  popularity_factor = user_popularity[user]\n",
    "  influence_factor = user_influence[user]\n",
    "  \n",
    "  # compute the social capital impact as the sum of mention connections, hashtag connections, resource sum, interaction sum,\n",
    "  # text similarity, popularity factor, and influence factor divided by the number of user connections\n",
    "  impact = (mention_connections + hashtag_connections + resource_sum + interaction_sum + similarity + popularity_factor + influence_factor) / user_connections\n",
    "  \n",
    "  return impact\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0ab0a4bd07e653e451a64cb3a171ddec94ddedb71f86f0f21941dd76a8744c36"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
