{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytwitter import Api\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "import math\n",
    "from pytwitter.models import User;\n",
    "from pytwitter.models import Tweet;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = Api(bearer_token=\"AAAAAAAAAAAAAAAAAAAAAMhqlAEAAAAA4Pqzn354Z5nlkP5lKaW98vzlVlA%3D7GIA03xacVKdFYTFg7qmgvWTZThpa2FFd4SNPUqP7uPK7Xjue5\")\n",
    "\n",
    "public_tweets = api.search_tweets(query=\"carnaval is:verified lang:pt has:hashtags -is:retweet\", expansions=[\"referenced_tweets.id.author_id\",\"in_reply_to_user_id\",\"attachments.media_keys\",\"author_id\",\"entities.mentions.username\"], \n",
    "                                  user_fields=[\"created_at\",\"entities\",\"id\",\"location\",\"name\",\"pinned_tweet_id\",\"profile_image_url\",\"protected\",\"public_metrics\",\"url\",\"username\",\"verified\"],\n",
    "                                  tweet_fields=[\"attachments\",\"author_id\",\"context_annotations\",\"created_at\",\"entities\",\"geo\",\"in_reply_to_user_id\",\"lang\",\"public_metrics\",\"reply_settings\",\"source\"], max_results=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User's influence and reputation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_score(text):\n",
    "    \"float: The sentiment score between -1.0 (negative) and 1.0 (positive)\"\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence Calculus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_influence(user: User):\n",
    "    follower_count = user.public_metrics.followers_count\n",
    "\n",
    "    # Get user's tweet count and average engagement rate\n",
    "    tweets = api.get_timelines(user.id, max_results=100, tweet_fields=[\"attachments\",\"author_id\",\"context_annotations\",\"created_at\",\"entities\",\"geo\",\"in_reply_to_user_id\",\"lang\",\"public_metrics\",\"reply_settings\",\"source\"])\n",
    "    tweet_count = len(tweets.data)\n",
    "    total_engagement = 0\n",
    "    \n",
    "    for tweet in tweets.data:\n",
    "        total_engagement += tweet.public_metrics.like_count + tweet.public_metrics.retweet_count + tweet.public_metrics.quote_count + tweet.public_metrics.reply_count\n",
    "        \n",
    "    if tweet_count > 0:\n",
    "        avg_engagement_rate = total_engagement / (tweet_count * follower_count) if total_engagement > 0 and tweet_count > 0 and follower_count > 0 else 0\n",
    "    else:\n",
    "        avg_engagement_rate = 0\n",
    "\n",
    "    # Calculate influence score\n",
    "    influence_score = math.log(follower_count + 1, 10) * (avg_engagement_rate + 1)\n",
    "    \n",
    "    return influence_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reputation Calculus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reputation(user: User):\n",
    "    # Get user's recent mentions and replies\n",
    "    mentions = api.search_tweets(query=f\"@{user.username}\", max_results=100)\n",
    "    replies = api.search_tweets(query=f\"to:{user.username}\", max_results=100)\n",
    "\n",
    "    # Calculate reputation score based on sentiment analysis of mentions and replies\n",
    "    positive_sentiments = 0\n",
    "    negative_sentiments = 0\n",
    "    \n",
    "    for mention in mentions.data:\n",
    "        if mention.author_id != user.id:\n",
    "            sentiment = get_sentiment_score(mention.text)\n",
    "            if sentiment > 0:\n",
    "                positive_sentiments += 1\n",
    "            elif sentiment < 0:\n",
    "                negative_sentiments += 1\n",
    "                \n",
    "    for reply in replies.data:\n",
    "        if reply.author_id != user.id:\n",
    "            sentiment = get_sentiment_score(reply.text)\n",
    "            if sentiment > 0:\n",
    "                positive_sentiments += 1\n",
    "            elif sentiment < 0:\n",
    "                negative_sentiments += 1\n",
    "                \n",
    "    if (positive_sentiments + negative_sentiments) > 0:\n",
    "        reputation_score = positive_sentiments / (positive_sentiments + negative_sentiments)\n",
    "    else:\n",
    "        reputation_score = 0\n",
    "\n",
    "    # Return influence and reputation scores\n",
    "    return reputation_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Paulo\n",
      "[nltk_data]     Roberto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Paulo\n",
      "[nltk_data]     Roberto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Paulo\n",
      "[nltk_data]     Roberto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Paulo\n",
      "[nltk_data]     Roberto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def remove_urls(text):\n",
    "    \"\"\"Remove URLs from a text\"\"\"\n",
    "    return re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "def remove_mentions(text):\n",
    "    \"\"\"Remove mentions from a text\"\"\"\n",
    "    return re.sub(r\"@\\S+\", \"\", text)\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    \"\"\"Remove hashtags from a text\"\"\"\n",
    "    return re.sub(r\"#\\S+\", \"\", text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"Remove punctuation from a text\"\"\"\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Tokenize a text\"\"\"\n",
    "    return word_tokenize(text, language='portuguese')\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Remove stopwords from a list of tokens\"\"\"\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    return [token for token in tokens if not token in stop_words]\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    \"\"\"Lemmatize a list of tokens\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for token in tokens:\n",
    "        lemma = lemmatizer.lemmatize(token, wordnet.VERB)\n",
    "        if lemma == token:\n",
    "            lemma = lemmatizer.lemmatize(token, wordnet.NOUN)\n",
    "        if lemma == token:\n",
    "            lemma = lemmatizer.lemmatize(token, wordnet.ADJ)\n",
    "        if lemma == token:\n",
    "            lemma = lemmatizer.lemmatize(token, wordnet.ADV)\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def synonymize(tokens):\n",
    "    \"\"\"Synonymize a list of tokens\"\"\"\n",
    "    synonyms = []\n",
    "    for token in tokens:\n",
    "        synsets = wordnet.synsets(token, lang='por')\n",
    "        if synsets:\n",
    "            synset = synsets[0]\n",
    "            for lemma in synset.lemmas(lang='por'):\n",
    "                synonym = lemma.name()\n",
    "                if synonym not in synonyms and synonym != token:\n",
    "                    synonyms.append(synonym)\n",
    "    return synonyms\n",
    "\n",
    "def preprocess_tweet(text):\n",
    "    \"\"\"Preprocess a Brazilian Portuguese tweet\"\"\"\n",
    "    text = text.lower()\n",
    "    text = remove_urls(text)\n",
    "    text = remove_mentions(text)\n",
    "    text = remove_hashtags(text)\n",
    "    text = remove_punctuation(text)\n",
    "    tokens = tokenize(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = lemmatize(tokens)\n",
    "    tokens.extend(synonymize(tokens))\n",
    "    return tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Social Capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "\n",
    "def get_tweet_social_capital(tweet: Tweet):\n",
    "    # Get tweet text\n",
    "    text = tweet.text\n",
    "    \n",
    "    # Get number of likes and retweets\n",
    "    likes = tweet.public_metrics.like_count\n",
    "    retweets = tweet.public_metrics.retweet_count\n",
    "    replies = tweet.public_metrics.reply_count\n",
    "    quotes = tweet.public_metrics.quote_count\n",
    "    hashtags = len(re.findall(r'#(\\w+)', text))\n",
    "    words = len(preprocess_tweet(text))\n",
    "    \n",
    "    words = words if hashtags > words else words * 5\n",
    "    \n",
    "    # Get tweet creation time\n",
    "    created_at = tweet.created_at\n",
    "    created_at = datetime.datetime.strptime(created_at, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    now = datetime.datetime.utcnow()\n",
    "    age = (now - created_at).total_seconds() / 3600 # tweet age in hours\n",
    "    \n",
    "    # Calculate recency score\n",
    "    recency_score = math.exp(-0.1 * age)\n",
    "    \n",
    "    # Get URLs in tweet\n",
    "    urls = re.findall('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', text)\n",
    "    num_urls = len(urls)\n",
    "    \n",
    "    # Get number of emojis in tweet\n",
    "    emojis = re.findall(r'[^\\w\\s,]', text)\n",
    "    num_emojis = len(emojis)\n",
    "    \n",
    "    # Get number of photos and videos in tweet\n",
    "    num_medias = 0\n",
    "    \n",
    "    if tweet.attachments is not None and tweet.attachments.media_keys is not None:\n",
    "        for attachment in tweet.attachments.media_keys:\n",
    "            num_medias += 1\n",
    "    \n",
    "    # Calculate media score\n",
    "    media_score = 0.5 * num_medias\n",
    "    \n",
    "    # Calculate URL score\n",
    "    url_score = 0.5 * num_urls\n",
    "    \n",
    "    # Calculate emoji score\n",
    "    emoji_score = 0.5 * num_emojis\n",
    "    \n",
    "    # Calculate engagement score\n",
    "    engagement_score = likes + retweets + replies + quotes\n",
    "    \n",
    "    # Calculate social capital\n",
    "    social_capital = recency_score * (media_score + url_score + emoji_score + engagement_score + hashtags + words)\n",
    "    \n",
    "    return {'tweet': tweet, 'score': social_capital }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Capital Calculus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = {}\n",
    "for user in public_tweets.includes.users:\n",
    "    teste[user.id] = [calculate_influence(user), calculate_reputation(user)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CNNBrasil': 7.152619498308262, 'radiobandnewsfm': 7.008128073673878, 'RevistaISTOE': 6.893159213031416, 'BomDiaBrasil': 6.716933460417127, 'jornaldarecord': 6.702706951781267, 'jornalodia': 6.623824474514334, 'g1rio': 6.455287867400853, 'correio24horas': 6.452392144546988, 'g1bahia': 5.752073780712917, 'falabrasil': 5.733561311945156, 'santos_agora': 5.7202895322360385, 'colaboraprojeto': 5.312955634844281, 'recordtvminas': 5.120710182075079, 'irmaospiologo': 5.097951440671946, 'g1alagoas': 3.977033684182141, 'g1zonadamata': 3.7368676117484414}\n"
     ]
    }
   ],
   "source": [
    "ranking = {}\n",
    "usersTeste = {}\n",
    "for tweet in public_tweets.data:\n",
    "    for bla in public_tweets.includes.users:\n",
    "        if(bla.id == tweet.author_id):\n",
    "            usersTeste[bla.username] = sum(teste[tweet.author_id])\n",
    "            \n",
    "    ranking[tweet.id] = get_tweet_social_capital(tweet)\n",
    "    ranking[tweet.id]['score'] += sum(teste[tweet.author_id]) \n",
    "    \n",
    "    \n",
    "rankedUsers = dict(sorted(usersTeste.items(), key=lambda item: item[1], reverse=True))\n",
    "print(rankedUsers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246.58052393397892 #RESTAURA√á√ÉO üë∑‚Äç‚ôÇüèó TEATRO MUNICIPAL ENTRA EM NOVA ETAPA DE OBRAS! A restaura√ß√£o das fachadas do conjunto arquitet√¥nico do Centro de Cultura Patr√≠cia Galv√£o se inicia agora, ap√≥s o Carnaval. https://t.co/Xtdc5lHmSQ\n",
      "245.17151266797143 'N√£o podemos controlar comportamento', diz Muquiranas ap√≥s casos de ass√©dio e vandalismo\n",
      "\n",
      "Imagens de ass√©dio e destrui√ß√£o envolvendo foli√µes do bloco foram divulgadas no Carnaval. Bloco diz que faz campanhas de conscientiza√ß√£o https://t.co/Mx01FH96Zp #correio24h https://t.co/gfKYZAscu8\n",
      "178.1962867821083 O Minist√©rio dos Transportes ajudar√° o governo de S√£o Paulo a reconstruir as rodovias que foram danificadas por conta das fortes chuvas durante o Carnaval. A determina√ß√£o atende o pedido do governador do estado, Tarc√≠sio de Freitas (Republicanos) #CNNNovoDia https://t.co/lojyEVYASu\n",
      "168.17899019825933 Veja no #BomDiaBrasil: as buscas pelos desaparecidos em S√£o Sebasti√£o, no litoral de S√£o Paulo, entram nesta quinta-feira (23) no 5¬∫ dia. E mais: a festa da Imperatriz Leopoldinense, campe√£ do carnaval carioca depois de 22 anos. https://t.co/xX4EmIWWAu\n",
      "166.5759736627684 CARNAVAL 2023 | Escola de S√£o Gon√ßalo buscou compensar a emiss√£o de carbono e reduziu o uso de penas reais nas fantasias. Integrantes da @GRESUPP  tamb√©m participaram de mutir√£o para plantar mudas de esp√©cies.  #ColaboranoCarnaval\n",
      "https://t.co/FsBC9yQa1M\n",
      "132.7075002700884 Ao vivo no #FalaBrasil: mergulhadores recolhem lixo descartado no mar ap√≥s o carnaval\n",
      "\n",
      "‚û° Assista no @sigaplayplus: https://t.co/s97dShrAKf https://t.co/sEKIg8Hz7b\n",
      "122.49391465828948 Homem invade o carnaval de rua em Quissam√£ com um trator.\n",
      "Cr√©ditos: Reprodu√ß√£o Rede Social\n",
      "\n",
      "#ODia https://t.co/BSonPtT24Z\n",
      "120.72475912522404 Roseilde dos Santos, 49 anos, foi atropelada na ter√ßa-feira de carnaval e morreu no dia seguinte no HGE. https://t.co/huCI9CzVA6 #g1al\n",
      "116.40548042886955 'Comemorar e curtir' | Quadra da Real Grandeza recebe festa do t√≠tulo do carnaval em JF #g1 #g1zonadamata https://t.co/xjhp1KKYJC\n",
      "115.70438625340313 Desempenho dos hot√©is de Salvador supera n√∫meros pr√©-pandemia\n",
      "\n",
      "Prefeitura e trade tur√≠stico comemoram ocupa√ß√£o hoteleira acima de 96% no Carnaval 2023 https://t.co/dWAJ7i1LdX #correio24h https://t.co/M3KdJsjJIK\n",
      "111.38220798283976 Adriel Martins, de 17 anos,  foi morto durante briga em um bloco de carnaval; suspeito ainda n√£o foi localizado \n",
      "\n",
      "Confira: https://t.co/Th4WnpMRca #R7Minas\n",
      "110.21003750884584 Gar√ßons e funcion√°rios de bares e restaurantes da cidade ca√≠ram na folia para se despedir do carnaval na noite de Quarta-feira de Cinzas. https://t.co/Ze2MrW0qH7 #g1al\n",
      "108.67565361358443 Situa√ß√£o de ass√©dio viralizou nas redes sociais e muitas pessoas pediram que respons√°veis pelo bloco As Muquiranas tomem provid√™ncias https://t.co/0k4LxLzdKu #g1 #g1ba\n",
      "108.01602732363732 Bombeiros fizeram mais de 800 resgates no local durante os cinco dias do feriad√£o ====&gt;https://t.co/4fjKjwzFbz #g1Rio\n",
      "103.67980828695157 Karina Amaro, de 33 anos, estava na capital fluminense para aproveitar o feriado de carnaval #g1 #g1zonadamata https://t.co/XUGOYbc6Tj\n",
      "87.15448818469169 Escola de samba conquistou o 7¬∫ lugar da S√©rie Ouro no Carnaval 2023. #S√£oClemente #ODia https://t.co/68DewNtc6m\n",
      "73.48965730730332 Crise no #Carnaval de  Veneza? A jornalista @patymoraesnobre foi ver de perto e conta como as g√¥ndolas s√£o feitas \n",
      "https://t.co/Hh6nUwGyGF\n",
      "60.857955754412274 #ImperatrizLeopoldinense comemora t√≠tulo do #Carnaval de 2023: https://t.co/Gm8rqHt0RP\n",
      "57.44464981524451 Pedidos de licen√ßa m√©dica crescem no Carnaval; JR flagra venda de atestados falsos #JornalDaRecord #JR24H https://t.co/1YdCsAOB7n\n",
      "45.73149355844028 üéâüé≠üéÆ ASSISTA AGORA! SBT GAMES DE CARNAVAL!\n",
      "https://t.co/ZtG9S59HTv\n",
      "https://t.co/ZtG9S59HTv\n",
      "https://t.co/ZtG9S59HTv\n",
      "@sbt_games @sbtgamesoficial\n",
      "#carnaval #carnaval2023 #irmaospiologo #ps5 #nintendo #games #jogos #xbox #partoba #fail #atomicheart https://t.co/OFi0vxZ2pa\n"
     ]
    }
   ],
   "source": [
    "ranked = dict(sorted(ranking.items(), key=lambda item: item[1]['score'], reverse=True))\n",
    "for tweet in ranked: \n",
    "    print(ranking[tweet]['score'], ranking[tweet]['tweet'].text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence vs Interactions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reputation vs Interactions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram Interactions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_social_capital(tweets):\n",
    "    public_tweets = api.search_tweets(query=\"stf lang:pt has:hashtags is:retweet\", expansions=[\"referenced_tweets.id.author_id\",\"in_reply_to_user_id\",\"attachments.media_keys\",\"author_id\",\"entities.mentions.username\"],\n",
    "    user_fields=[\"created_at\",\"entities\",\"id\",\"location\",\"name\",\"pinned_tweet_id\",\"profile_image_url\",\"protected\",\"public_metrics\",\"url\",\"username\",\"verified\"],\n",
    "    tweet_fields=[\"attachments\",\"author_id\",\"context_annotations\",\"created_at\",\"entities\",\"geo\",\"in_reply_to_user_id\",\"lang\",\"public_metrics\",\"reply_settings\",\"source\"], max_results=100)\n",
    "\n",
    "    api.get_followers()\n",
    "\n",
    "    public_tweets.includes.tweets\n",
    "\n",
    "#analyze_social_capital(public_tweets.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def social_capital_impact(tweet, social_connections, interactions, texts, resources, user_popularity, user_influence):\n",
    "  # extract the user, mentions, and hashtags from the tweet\n",
    "  user = tweet['user']\n",
    "  mentions = tweet['mentions']\n",
    "  hashtags = tweet['hashtags']\n",
    "  \n",
    "  # compute the number of connections the user has in the social network\n",
    "  user_connections = np.sum(social_connections[user])\n",
    "  \n",
    "  # compute the number of connections between the user and the mentions\n",
    "  mention_connections = 0\n",
    "  for mention in mentions:\n",
    "    mention_connections += social_connections[user][mention]\n",
    "  \n",
    "  # compute the number of connections between the user and the hashtags\n",
    "  hashtag_connections = 0\n",
    "  for hashtag in hashtags:\n",
    "    hashtag_connections += social_connections[user][hashtag]\n",
    "  \n",
    "  # compute the sum of the resources associated with the tweet\n",
    "  resource_sum = 0\n",
    "  for resource in resources:\n",
    "    if resource in tweet['resources']:\n",
    "      resource_sum += resources[resource]\n",
    "  \n",
    "  # compute the number of interactions for the tweet\n",
    "  interaction_sum = interactions[tweet['tweet_id']]\n",
    "  \n",
    "  # compute the text similarity between the tweet and the other texts\n",
    "  vectorizer = TfidfVectorizer()\n",
    "  tweet_text = [tweet['text']]\n",
    "  tweet_vector = vectorizer.fit_transform(tweet_text)\n",
    "  other_vectors = vectorizer.transform(texts)\n",
    "  similarity = cosine_similarity(tweet_vector, other_vectors)\n",
    "  \n",
    "  # compute the user popularity and influence factors\n",
    "  popularity_factor = user_popularity[user]\n",
    "  influence_factor = user_influence[user]\n",
    "  \n",
    "  # compute the social capital impact as the sum of mention connections, hashtag connections, resource sum, interaction sum,\n",
    "  # text similarity, popularity factor, and influence factor divided by the number of user connections\n",
    "  impact = (mention_connections + hashtag_connections + resource_sum + interaction_sum + similarity + popularity_factor + influence_factor) / user_connections\n",
    "  \n",
    "  return impact\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0ab0a4bd07e653e451a64cb3a171ddec94ddedb71f86f0f21941dd76a8744c36"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
